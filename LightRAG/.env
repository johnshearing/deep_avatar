# This file is required by the LightRAG server.
# If any changes are made the reload the file into memory with the following command at the bash terminal before using LighRAG Server.
# source .env

WORKING_DIR=/home/js/lgt/LightRAG/_0_jack_work_dir_01
INPUT_DIR=/home/js/lgt/LightRAG/_0_jack_processed

LLM_BINDING=openai

# Larger and more expensive LLM which likely gives better results.
# LLM_MODEL=gpt-4o
# Smaller and less expensive LLM which likely does not respond as well as the larger model.
# In anycase, this model gives very good performance for the money spent.
# At some point I will start to use local LLMs to avoid expenses entirely. 
# Electricity and equipment costs will go up, of course, when using a local LLM.
LLM_MODEL=gpt-4o-mini

LLM_BINDING_HOST=https://api.openai.com/v1
# LLM_BINDING_API_KEY="Your_API_Key here if not specified already in your .bashrc file" 

EMBEDDING_BINDING=openai
EMBEDDING_MODEL=text-embedding-3-large
EMBEDDING_DIM=3072
EMBEDDING_BINDING_HOST=https://api.openai.com/v1
# EMBEDDING_BINDING_API_KEY="Your_API_Key here if not specified already in your .bashrc file"


### Number of worker processes, not greater than (2 x number_of_cores) + 1
WORKERS=2
### Number of parallel files to process in one batch
MAX_PARALLEL_INSERT=2
### Max concurrent requests to the LLM
MAX_ASYNC=2

### Max concurrency requests for Embedding
### I dropped this from 16 to 2 because of rate limit errors with OpenAI
EMBEDDING_FUNC_MAX_ASYNC=2

MAX_GRAPH_NODES=20000

MAX_TOKEN_SIZE=8192

MAX_TOKENS=32768

MAX_TOKEN_SUMMARY=500

COSINE_THRESHOLD=.2

TOP_K=60


